{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWP33h2qol4H6mID/kdPZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaurangRawat/Langchain/blob/main/GROQ_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGHAjPy9RbIz"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import OllamaEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "import time\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "## load the Groq API key\n",
        "groq_api_key=os.environ['GROQ_API_KEY']\n",
        "\n",
        "if \"vector\" not in st.session_state:\n",
        "    st.session_state.embeddings=OllamaEmbeddings()\n",
        "    st.session_state.loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
        "    st.session_state.docs=st.session_state.loader.load()\n",
        "\n",
        "    st.session_state.text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "    st.session_state.final_documents=st.session_state.text_splitter.split_documents(st.session_state.docs[:50])\n",
        "    st.session_state.vectors=FAISS.from_documents(st.session_state.final_documents,st.session_state.embeddings)\n",
        "\n",
        "st.title(\"ChatGroq Demo\")\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,\n",
        "             model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "prompt=ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "Answer the questions based on the provided context only.\n",
        "Please provide the most accurate response based on the question\n",
        "<context>\n",
        "{context}\n",
        "<context>\n",
        "Questions:{input}\n",
        "\n",
        "\"\"\"\n",
        ")\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "retriever = st.session_state.vectors.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "prompt=st.text_input(\"Input you prompt here\")\n",
        "\n",
        "if prompt:\n",
        "    start=time.process_time()\n",
        "    response=retrieval_chain.invoke({\"input\":prompt})\n",
        "    print(\"Response time :\",time.process_time()-start)\n",
        "    st.write(response['answer'])\n",
        "\n",
        "    # With a streamlit expander\n",
        "    with st.expander(\"Document Similarity Search\"):\n",
        "        # Find the relevant chunks\n",
        "        for i, doc in enumerate(response[\"context\"]):\n",
        "            st.write(doc.page_content)\n",
        "            st.write(\"--------------------------------\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llama3.py\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "## load the GROQ And OpenAI API KEY\n",
        "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
        "groq_api_key=os.getenv('GROQ_API_KEY')\n",
        "\n",
        "st.title(\"Chatgroq With Llama3 Demo\")\n",
        "\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,\n",
        "             model_name=\"Llama3-8b-8192\")\n",
        "\n",
        "prompt=ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "Answer the questions based on the provided context only.\n",
        "Please provide the most accurate response based on the question\n",
        "<context>\n",
        "{context}\n",
        "<context>\n",
        "Questions:{input}\n",
        "\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def vector_embedding():\n",
        "\n",
        "    if \"vectors\" not in st.session_state:\n",
        "\n",
        "        st.session_state.embeddings=OpenAIEmbeddings()\n",
        "        st.session_state.loader=PyPDFDirectoryLoader(\"./us_census\") ## Data Ingestion\n",
        "        st.session_state.docs=st.session_state.loader.load() ## Document Loading\n",
        "        st.session_state.text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200) ## Chunk Creation\n",
        "        st.session_state.final_documents=st.session_state.text_splitter.split_documents(st.session_state.docs[:20]) #splitting\n",
        "        st.session_state.vectors=FAISS.from_documents(st.session_state.final_documents,st.session_state.embeddings) #vector OpenAI embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "prompt1=st.text_input(\"Enter Your Question From Doduments\")\n",
        "\n",
        "\n",
        "if st.button(\"Documents Embedding\"):\n",
        "    vector_embedding()\n",
        "    st.write(\"Vector Store DB Is Ready\")\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "if prompt1:\n",
        "    document_chain=create_stuff_documents_chain(llm,prompt)\n",
        "    retriever=st.session_state.vectors.as_retriever()\n",
        "    retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
        "    start=time.process_time()\n",
        "    response=retrieval_chain.invoke({'input':prompt1})\n",
        "    print(\"Response time :\",time.process_time()-start)\n",
        "    st.write(response['answer'])\n",
        "\n",
        "    # With a streamlit expander\n",
        "    with st.expander(\"Document Similarity Search\"):\n",
        "        # Find the relevant chunks\n",
        "        for i, doc in enumerate(response[\"context\"]):\n",
        "            st.write(doc.page_content)\n",
        "            st.write(\"--------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p-Vh1Vjd4TBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "import cassio\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "CBhqWNEF4T33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key=os.environ['GROQ_API_KEY']\n",
        "\n",
        "## connection of the ASTRA DB\n",
        "ASTRA_DB_APPLICATION_TOKEN=\"AstraCS:mifTZmAApXkXyN:8ae5390b5181dbf4ed575bbc3ccddbb9845300cb7071b9f98bf3ba72cbca837c\" # enter the \"AstraCS:...\" string found in in your Token JSON file\"\n",
        "ASTRA_DB_ID=\"31d5fd09-8c1f-c-aee0bda20405\"\n",
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN,database_id=ASTRA_DB_ID)"
      ],
      "metadata": {
        "id": "39w_1Ix74uNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
        "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
        "\n",
        "                     )))\n",
        "\n",
        "text_documents=loader.load()"
      ],
      "metadata": {
        "id": "nM6CxOMj4wgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "docs=text_splitter.split_documents(text_documents)\n",
        "docs[:5]"
      ],
      "metadata": {
        "id": "SEL03Ctc43ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert Data Into Vectors and store in AstraDB\n",
        "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
        "embeddings=OpenAIEmbeddings()\n",
        "astra_vector_store=Cassandra(\n",
        "    embedding=embeddings,\n",
        "    table_name=\"qa_mini_demo\",\n",
        "    session=None,\n",
        "    keyspace=None\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "P8jOK2MO453o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "astra_vector_store.add_documents(docs)\n",
        "print(\"Inserted %i headlines.\" % len(docs))\n",
        "\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ],
      "metadata": {
        "id": "jE9OOsDJ4-Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatGroq(groq_api_key=groq_api_key,\n",
        "         model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based only on the provided context.\n",
        "Think step by step before providing a detailed answer.\n",
        "I will tip you $1000 if the user finds the answer helpful.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")"
      ],
      "metadata": {
        "id": "L_zuBspC5Ah6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "astra_vector_index.query(\"Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique\",llm=llm)\n"
      ],
      "metadata": {
        "id": "yGSygfds5DWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "retriever=astra_vector_store.as_retriever()\n",
        "document_chain=create_stuff_documents_chain(llm,prompt)\n",
        "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
      ],
      "metadata": {
        "id": "nHxsw8rJ5FMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=retrieval_chain.invoke({\"input\":\"Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique\"})\n",
        "response"
      ],
      "metadata": {
        "id": "vcivQF_B5H7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"answer\"]"
      ],
      "metadata": {
        "id": "6GbxZpZW5Kt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}